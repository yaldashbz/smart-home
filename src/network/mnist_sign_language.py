"""mnist_sign_language.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jwPzt5QIFGDm1NERVCoci4Yl6tJYwaHN
"""

import pandas as pd
import tensorflow as tf
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout
from tensorflow.keras.models import Sequential

train_df = pd.read_csv('./sign_mnist_train.csv')

# Rename label into Label
train_df.rename(columns={'label': 'Label'}, inplace=True)
# Shuffle
train_df = train_df.sample(frac=1.0).reset_index(drop=True)

# The labels are coded in numbers. 
# Create a mapping to get the letters corresponding to the numbers
alphab = 'abcdefghijklmnopqrstuvwxyz'
mapping_letter = {}

for i, l in enumerate(alphab):
    mapping_letter[l] = i
mapping_letter = {v: k for k, v in mapping_letter.items()}

train_df_original = train_df.copy()

# Split into training, and validation sets
val_index = int(train_df.shape[0] * 0.1)

train_df = train_df_original.iloc[val_index:]
val_df = train_df_original.iloc[:val_index]

y_train = train_df['Label']
y_val = val_df['Label']

# Reshape the traing and test set to use them with a generator
X_train = train_df.drop('Label', axis=1).values.reshape(train_df.shape[0], 28, 28, 1)
X_val = val_df.drop('Label', axis=1).values.reshape(val_df.shape[0], 28, 28, 1)

# Data augmentation
generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255,
                                                            rotation_range=10,
                                                            zoom_range=0.10,
                                                            width_shift_range=0.1,
                                                            height_shift_range=0.1,
                                                            shear_range=0.1,
                                                            horizontal_flip=False,
                                                            fill_mode="nearest")

X_train_flow = generator.flow(X_train, y_train, batch_size=32)
X_val_flow = generator.flow(X_val, y_val, batch_size=32)

model = Sequential([Conv2D(filters=32, kernel_size=(3, 3), activation="relu", input_shape=(28, 28, 1)),
                    MaxPool2D(2, 2, padding='same'),

                    Conv2D(filters=128, kernel_size=(3, 3), activation="relu"),
                    MaxPool2D(2, 2, padding='same'),

                    Conv2D(filters=512, kernel_size=(3, 3), activation="relu"),
                    MaxPool2D(2, 2, padding='same'),

                    Flatten(),

                    Dense(units=1024, activation="relu"),
                    Dense(units=256, activation="relu"),
                    Dropout(0.5),
                    Dense(units=25, activation="softmax")])

model.compile(optimizer='adam', loss="sparse_categorical_crossentropy", metrics=["accuracy"])

model.summary()

learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=2, verbose=1, factor=0.5, min_lr=0.00001)

history = model.fit(X_train_flow,
                    validation_data=X_val_flow,
                    epochs=100,
                    callbacks=[
                        tf.keras.callbacks.EarlyStopping(
                            monitor='val_loss',
                            patience=5,
                            restore_best_weights=True),

                        learning_rate_reduction])

model.save('sign_language_cnn.h5')

model = tf.keras.models.load_model(r"./sign_language_cnn.h5")
